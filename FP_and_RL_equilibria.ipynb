{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14f5fe7-5fed-47be-a6ea-967e5db5a29f",
   "metadata": {},
   "source": [
    "# Fictitious play and reinforcement learning for computing equilibria\n",
    "- Repeated & zero sum (stochastic) games\n",
    "- Fictitious play (FP) (theory and implementation)\n",
    "- Reinforcement Learning (RL) (theory and implementation)\n",
    "- Experimental results comparing FP and RL\n",
    "\n",
    "---\n",
    "\n",
    "## Repeated & zero sum (stochastic) games\n",
    "\n",
    "### Zero-sum game\n",
    "**Situation**\n",
    "- competing entities\n",
    "- the result is an **advantage for one side** and an **equivalent loss** for the other (+5, -5)\n",
    "- the net improvement in benefit of the whole game is zero.\n",
    "    - Why? The advantage for one side is an equal loss for the other side.\n",
    "    - This sum is zero.\n",
    "\n",
    "#### Examples of games\n",
    "- poker\n",
    "- chess\n",
    "- sports?\n",
    "- bridge\n",
    "\n",
    "### Repeated/Iterated game\n",
    "- The same stage is played at each timeframe.\n",
    "- Repeated number of repetitions of a base game (stage game)\n",
    "- Same game, multiple games\n",
    "\n",
    "#### Examples\n",
    "- 2 gas stations offer competing prices\n",
    "- Stage Game: The single-shot game played in each period.\n",
    "\n",
    "### Stochastic/Markov games\n",
    "- dynamic, multi-agent\n",
    "- probabilities influence the transition to next state\n",
    "- player strategies depend only on the current state\n",
    "\n",
    "## Environment/Game description\n",
    "\n",
    "Construct a game/environment which combines the above 3 characteristics.\n",
    "In other words, it asks for a Competitive Markov Game, a two-player zero-sum game.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Equilibria\n",
    "\n",
    "### Nash equilibrium\n",
    "- no player could gain more by changing their play strategy in a game.\n",
    "- players choose their optimal strategy against the other players constant strategy\n",
    "- does not guarantee overall best pay-off\n",
    "- suboptimal for the group\n",
    "- **Pareto inefficient**\n",
    "\n",
    "\n",
    "### Pareto optimality/efficiency\n",
    "- is a group/collective strategy\n",
    "- every player is better off\n",
    "- no players in worse situations (aka no punished players)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Fictitious play - FP\n",
    "- players keep track of the empirical frequency of opponent's past moves\n",
    "    - i.e. player 2 played heads 60% of the time\n",
    "    - choose best response against that average\n",
    "- players adjust their strategies\n",
    "- players assume opponent's strategy based on past historical frequency\n",
    "- FP is guaranteed to converge to Nash equilibrium.\n",
    "    - WHY?\n",
    "    - Player 2, always chooses best/rational response against player 1's move\n",
    "- FP approach can be systematically exploited!!!\n",
    "\n",
    "**Refs**:\n",
    "What's FP:\n",
    "- https://en.wikipedia.org/wiki/Fictitious_play\n",
    "\n",
    "FP Agent design:\n",
    "- https://www.youtube.com/watch?v=_XIdEr-wtJg\n",
    "\n",
    "FP Agent beliefs initialization:\n",
    "- https://cse.unl.edu/~lksoh/Classes/CSCE475_875_Fall17/handouts/sup09.pdf\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Reinforcement Learning - RL\n",
    "- Player explores actions and receives rewards or punishments - feedback\n",
    "- Implemented via Q-learning or variants.\n",
    "- Q-value updated on trial and error - exploration\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "\n",
    "### 1. Beefy Rock-Paper-Scissors - RPS\n",
    "\n",
    "This is a high stakes Rock-Paper-Scissors game.\n",
    "\n",
    "#### States\n",
    "1. `State-i`: Initial state: Standard rewards (+1 win, -1 loss)\n",
    "2. `State-n`: Next state: Rewards of previous state are doubled each time\n",
    "3. We cap the states to a number to reduce the amount of memory for the agents\n",
    "\n",
    "#### Stochastic transition (P)\n",
    "- If there's a draw at any state, there's a P-chance to transition into next state in which rewards double.\n",
    "- Otherwise, game stays in the same state.\n",
    "\n",
    "#### Zero-Sum rewards (R)\n",
    "- The sum of the rewards for Player 1 and Player 2 is always 0.\n",
    "- One player wins, other loses: rewards for one is equal penalty for other.\n",
    "\n",
    "#### Repeated\n",
    "- Player play 10 rounds of the same game.\n",
    "\n",
    "---\n",
    "\n",
    "# DONE\n",
    "\n",
    "- [x] imlement beefy RPS env\n",
    "- [x] implement FP agent\n",
    "- [x] implement q-learning agent\n",
    "- [x] Move into notebook\n",
    "\n",
    "# TODO\n",
    "\n",
    "- [ ] implement **decaying** epsilon\n",
    "\n",
    "```python\n",
    "np.linspace(1, 0, 1000)\n",
    "```\n",
    "\n",
    "- [ ] **implement minmax q-learning**\n",
    "\n",
    "- [ ] Convert into stochastic with limit cap\n",
    "- [ ] break into modules\n",
    "- [ ] extract metrics [scores, env.state, q-table, sigma, epsilon, diagrams, experiments, interpretations, self-plays]\n",
    "- [ ] use petting zoo for RPS\n",
    "- [ ] research - competitive grid world\n",
    "- [ ] research - implement pong game (discrete actions)\n",
    "- [ ] research - hunter-pray game gridworld\n",
    "- [ ] research - implement multi-agent shooting game\n",
    "- [ ] implement SARSA agent - https://www.geeksforgeeks.org/machine-learning/sarsa-reinforcement-learning/\n",
    "- [ ] practical application ???\n",
    "- [ ] evolutionary algorithms - research\n",
    "- [ ] bomb difusal game/environment\n",
    "- [ ] penalties game/environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18cb80-924f-438b-83c6-68111ade10e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5182eb2-a404-426a-91e9-2360116bd60b",
   "metadata": {},
   "source": [
    "# Rock-Paper-Scissors environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86135a70-58b4-4985-b993-3e5472d2853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RPS_environment():\n",
    "    \"\"\"\n",
    "    Rock Paper Scissors environment - high stakes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payoff_matrix=None, stochastic=False, transition_prob=0.3, max_states=3):\n",
    "\n",
    "        # Payoff Matrix: Player 1 rows, Player 2 cols\n",
    "        # 0: Rock\n",
    "        # 1: Paper\n",
    "        # 2: Scissors\n",
    "        # self.matrix = [\n",
    "        #         # R  P  S\n",
    "        #         [0, -1, 1],  # R\n",
    "        #         [1, 0, -1],  # P\n",
    "        #         [-1, 1, 0],  # S\n",
    "        #         ]\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "        self.stochastic = stochastic\n",
    "        self.state = 0\n",
    "        self.P = transition_prob\n",
    "        self.max_states = max_states\n",
    "\n",
    "    def step(self, action1, action2):\n",
    "        \"\"\"\n",
    "        Transition function:\n",
    "        1. receive actions from players\n",
    "        2. calculate rewards\n",
    "        3. update state\n",
    "        4. return information\n",
    "        \"\"\"\n",
    "\n",
    "        reward1 = self.payoff_matrix[action1][action2]*(self.state+1)\n",
    "        reward2 = -reward1\n",
    "\n",
    "        # Calculate transition\n",
    "        if self.stochastic:\n",
    "            if action1 == action2:\n",
    "                if random.random() < self.P:\n",
    "                    if self.state < self.max_states:\n",
    "                        # We transition\n",
    "                        self.state += 1\n",
    "            else:\n",
    "                self.reset()\n",
    "\n",
    "        return self.state, reward1, reward2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Winning condition reached.\"\"\"\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb753e1-343f-4872-a3d2-d5fa29c58e7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Fictitious play - FP agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80962388-bc4d-412c-a783-db033cd3a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FictitiousPlayAgent():\n",
    "    \"\"\"\n",
    "    Fictitious Play Agent: A model based agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payoff_matrix):\n",
    "        # Times a move has been played\n",
    "                      #R #P #S\n",
    "        self.counts = [1, 1, 1]\n",
    "\n",
    "        # Probability distribution\n",
    "                      #R #P #S\n",
    "        self.sigma = [0, 0, 0]\n",
    "\n",
    "        # Utility matrix\n",
    "        self.rows = 3\n",
    "        self.cols = 3\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "\n",
    "    def action(self, opponent_action):\n",
    "        \"\"\"\n",
    "        Calculate the best action and ACT.\n",
    "        \"\"\"\n",
    "        if opponent_action is not None:\n",
    "            self.update_counts(opponent_action)\n",
    "            self.update_sigma(opponent_action)\n",
    "            best_move = self.find_best_move()\n",
    "        else:\n",
    "            best_move = random.randint(0, 2)\n",
    "        return best_move\n",
    "\n",
    "    def update_counts(self, opponent_action):\n",
    "        self.counts[opponent_action] += 1\n",
    "\n",
    "    def update_sigma(self, opponent_action):\n",
    "        for i in range(len(self.sigma)):\n",
    "            self.sigma[i] = round(self.counts[i]/sum(self.counts), 3)\n",
    "\n",
    "    def find_best_move(self):\n",
    "        \"\"\"\n",
    "        Calculates utilities using distribution sigma and returns best move.\n",
    "        \"\"\"\n",
    "        max_u = -float(\"inf\")\n",
    "        best_move = random.randint(0, 2)\n",
    "        for i in range(self.rows):\n",
    "            row_i_sum = sum([a*b for a, b in zip(self.payoff_matrix[i], self.sigma)])\n",
    "            if row_i_sum > max_u:\n",
    "                max_u = row_i_sum\n",
    "                best_move = i\n",
    "\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf229d9-42e8-427f-beac-88ead2c3d18a",
   "metadata": {},
   "source": [
    "# Scenario: FP self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c19bfbea-9073-48ab-a64c-17f68cb81f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_self_play(rounds):\n",
    "    env = RPS_environment(\n",
    "        payoff_matrix=[\n",
    "                 # R  P  S\n",
    "                 [0, -1, 1],  # R\n",
    "                 [1, 0, -1],  # P\n",
    "                 [-1, 1, 0],  # S\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Row player\n",
    "    agent1 = FictitiousPlayAgent(\n",
    "            [\n",
    "                 #R  P  S\n",
    "                [0, -1, 1],  # R\n",
    "                [1, 0, -1],  # P\n",
    "                [-1, 1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Column player\n",
    "    agent2 = FictitiousPlayAgent(\n",
    "            [\n",
    "                #R  P  S\n",
    "                [0, 1, -1],  # R\n",
    "                [-1, 0, 1],  # P\n",
    "                [1, -1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Initializer\n",
    "    last_act2 = None\n",
    "    last_act1 = None\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(rounds):\n",
    "        curr_act1 = agent1.action(last_act2)\n",
    "        curr_act2 = agent2.action(last_act1)\n",
    "\n",
    "        env.step(curr_act1, curr_act2)\n",
    "\n",
    "        last_act1 = curr_act1\n",
    "        last_act2 = curr_act2\n",
    "\n",
    "    print(f\"Agent 1 counts: {agent1.counts}\\nsigma: {agent1.sigma}\")\n",
    "    print(f\"Agent 2 counts: {agent2.counts}\\nsigma: {agent2.sigma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282167c3-e5e8-47b4-bad5-471d870520aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 counts: [667, 1, 334]\n",
      "sigma: [0.666, 0.001, 0.333]\n",
      "Agent 2 counts: [667, 334, 1]\n",
      "sigma: [0.666, 0.333, 0.001]\n"
     ]
    }
   ],
   "source": [
    "fp_self_play(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab462b94-784d-494f-bcae-0a5e419097cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "334a3b60-d9b8-42a5-8765-d5539352a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearning():\n",
    "    \"\"\"Generic Q-Learning algorightm.\"\"\"\n",
    "\n",
    "    def __init__(self, states=[0], actions=[0, 1, 2]):\n",
    "        # Initialize Q-table\n",
    "        self.q_table = {state: [0.0 for action in actions] for state in states}\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.action_space = actions\n",
    "\n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using the epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # Exploire - Exploit\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state_values = self.q_table[state]\n",
    "            max_val = max(state_values)\n",
    "            # If we have multiple max values, runs a loop\n",
    "            best_action = [i for i, v in enumerate(state_values) if v == max_val]\n",
    "            return np.random.choice(best_action)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the Q-value using Bellman eq..\n",
    "        \"\"\"\n",
    "        max_next_q = max(self.q_table[next_state])\n",
    "        # TEmpoeral difference\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "\n",
    "        # New Q value\n",
    "        self.q_table[state][action] += self.alpha * (td_target - self.q_table[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "706cc878-1eaa-4389-9005-c4746807961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_vs_qlearning(rounds):\n",
    "    \n",
    "    env = RPS_environment(\n",
    "        payoff_matrix=[\n",
    "                 # R  P  S\n",
    "                 [0, -1, 1],  # R\n",
    "                 [1, 0, -1],  # P\n",
    "                 [-1, 1, 0],  # S\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Row agent\n",
    "    agent1 = FictitiousPlayAgent(\n",
    "            [\n",
    "                 #R  P  S\n",
    "                [0, -1, 1],  # R\n",
    "                [1, 0, -1],  # P\n",
    "                [-1, 1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    agent2 = QLearning()\n",
    "\n",
    "    # Initializer\n",
    "    last_act2 = None\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(rounds):\n",
    "\n",
    "        # Register actions\n",
    "        curr_act1 = agent1.action(last_act2)\n",
    "        curr_act2 = agent2.action(env.state)\n",
    "\n",
    "        # Step the environment\n",
    "        env_state, reward1, reward2 = env.step(curr_act1, curr_act2)\n",
    "\n",
    "        # RL agent learns\n",
    "        agent2.update_q_value(env_state, curr_act2, reward2, env_state)\n",
    "\n",
    "        last_act2 = curr_act2\n",
    "\n",
    "    print(f\"Agent 1 counts: {agent1.counts}\\nsigma: {agent1.sigma}\\n--------\")\n",
    "    print(f\"Agent 2 q_table: {agent2.q_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a06c04-ca0d-440a-99af-7781ea7e057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 counts: [339, 325, 338]\n",
      "sigma: [0.338, 0.324, 0.337]\n",
      "--------\n",
      "Agent 2 q_table: {0: [-0.0002295977363066387, -0.016748797941038056, 0.49823405831279405]}\n"
     ]
    }
   ],
   "source": [
    "fp_vs_qlearning(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46b52a-128c-41b2-bc84-2bd242a1553c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543150d0-e528-45c3-9490-ca08748fc22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f12b9-7bc6-44f3-b5c6-74c09db34bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a24ec-409b-4ffd-b7a9-a9b3124c8004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
