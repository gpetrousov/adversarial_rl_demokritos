{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14f5fe7-5fed-47be-a6ea-967e5db5a29f",
   "metadata": {},
   "source": [
    "# Fictitious play and reinforcement learning for computing equilibria\n",
    "- Repeated & zero sum (stochastic) games\n",
    "- Fictitious play (FP) (theory and implementation)\n",
    "- Reinforcement Learning (RL) (theory and implementation)\n",
    "- Experimental results comparing FP and RL\n",
    "- Reinforcement Learning (RL) (theory and implementation) Experimental results comparing FP and RL\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Repeated & zero sum (stochastic) games\n",
    "\n",
    "### Zero-sum game\n",
    "**Situation**\n",
    "- competing entities\n",
    "- the result is an **advantage for one side** and an **equivalent loss** for the other (+5, -5)\n",
    "- the net improvement in benefit of the whole game is zero.\n",
    "    - Why? The advantage for one side is an equal loss for the other side.\n",
    "    - This sum is zero.\n",
    "\n",
    "#### Examples of games\n",
    "- poker\n",
    "- chess\n",
    "- sports?\n",
    "- bridge\n",
    "\n",
    "### Repeated/Iterated game\n",
    "- The same stage is played at each timeframe.\n",
    "- Repeated number of repetitions of a base game (stage game)\n",
    "- Same game, multiple games\n",
    "\n",
    "#### Examples\n",
    "- 2 gas stations offer competing prices\n",
    "- Stage Game: The single-shot game played in each period.\n",
    "\n",
    "### Stochastic/Markov games\n",
    "- dynamic, multi-agent\n",
    "- probabilities influence the transition to next state\n",
    "- player strategies depend only on the current state\n",
    "\n",
    "## Environment/Game description\n",
    "\n",
    "Construct a game/environment which combines the above 3 characteristics.\n",
    "In other words, it asks for a Competitive Markov Game, a two-player zero-sum game.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Equilibria\n",
    "\n",
    "### Nash equilibrium\n",
    "- no player could gain more by changing their play strategy in a game.\n",
    "- players choose their optimal strategy against the other players constant strategy\n",
    "- does not guarantee overall best pay-off\n",
    "- suboptimal for the group\n",
    "- **Pareto inefficient**\n",
    "\n",
    "\n",
    "### Pareto optimality/efficiency\n",
    "- is a group/collective strategy\n",
    "- every player is better off\n",
    "- no players in worse situations (aka no punished players)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Fictitious play - FP\n",
    "- players keep track of the empirical frequency of opponent's past moves\n",
    "    - i.e. player 2 played heads 60% of the time\n",
    "    - choose best response against that average\n",
    "- players adjust their strategies\n",
    "- players assume opponent's strategy based on past historical frequency\n",
    "- FP is guaranteed to converge to Nash equilibrium.\n",
    "    - WHY?\n",
    "    - Player 2, always chooses best/rational response against player 1's move\n",
    "- FP approach can be systematically exploited!!!\n",
    "\n",
    "**Refs**:\n",
    "What's FP:\n",
    "- https://en.wikipedia.org/wiki/Fictitious_play\n",
    "\n",
    "FP Agent design:\n",
    "- https://www.youtube.com/watch?v=_XIdEr-wtJg\n",
    "\n",
    "FP Agent beliefs initialization:\n",
    "- https://cse.unl.edu/~lksoh/Classes/CSCE475_875_Fall17/handouts/sup09.pdf\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Reinforcement Learning - RL\n",
    "- Player explores actions and receives rewards or punishments - feedback\n",
    "- Implemented via Q-learning or variants.\n",
    "- Q-value updated on trial and error - exploration\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Environment 1. Simple Rock Paper Scissors - RPS\n",
    "\n",
    "- Same stage played at each iteration.\n",
    "\n",
    "### Environment 2. Beefy Rock-Paper-Scissors - BRPS\n",
    "\n",
    "This is a high stakes Rock-Paper-Scissors game.\n",
    "\n",
    "#### States\n",
    "1. `State-i`: Initial state: Standard rewards (+1 win, -1 loss)\n",
    "2. `State-n`: Next state: Rewards of previous state are doubled each time\n",
    "3. We cap the states to a number to reduce the amount of memory for the agents\n",
    "\n",
    "#### Stochastic transition (P)\n",
    "- If there's a draw at any state, there's a P-chance to transition into next state in which rewards double.\n",
    "- Otherwise, game stays in the same state.\n",
    "\n",
    "#### Zero-Sum rewards (R)\n",
    "- The sum of the rewards for Player 1 and Player 2 is always 0.\n",
    "- One player wins, other loses: rewards for one is equal penalty for other.\n",
    "\n",
    "#### Repeated\n",
    "- Player play 10 rounds of the same game.\n",
    "\n",
    "---\n",
    "\n",
    "# DONE\n",
    "\n",
    "- [x] imlement beefy RPS env\n",
    "- [x] implement FP agent\n",
    "- [x] implement q-learning agent\n",
    "- [x] Move into notebook\n",
    "\n",
    "# TODO\n",
    "\n",
    "- [ ] implement **decaying** epsilon\n",
    "\n",
    "```python\n",
    "np.linspace(1, 0, 1000)\n",
    "```\n",
    "\n",
    "- [ ] **implement minmax q-learning** - https://github.com/tocom242242/minimax_q_learning\n",
    "\n",
    "- [ ] Convert into stochastic with limit cap\n",
    "- [ ] break into modules\n",
    "- [ ] extract metrics [scores, env.state, q-table, sigma, epsilon, diagrams, experiments, interpretations, self-plays]\n",
    "- [ ] use petting zoo for RPS\n",
    "- [ ] research - competitive grid world\n",
    "- [ ] research - implement pong game (discrete actions)\n",
    "- [ ] research - hunter-pray game gridworld\n",
    "- [ ] research - implement multi-agent shooting game\n",
    "- [ ] implement SARSA agent - https://www.geeksforgeeks.org/machine-learning/sarsa-reinforcement-learning/\n",
    "- [ ] practical application ???\n",
    "- [ ] evolutionary algorithms - research\n",
    "- [ ] bomb difusal game/environment\n",
    "- [ ] penalties game/environment\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "READ THIS FIRST: https://lefkippos.ds.unipi.gr/modules/document/file.php/AI_IIT113/Lectures%205%20%26%206%20Agents-Interactions_Game_Theory.pdf\n",
    "\n",
    "- Q-Learning (Pong) - https://courses.grainger.illinois.edu/ece448/sp2018/assignment4.html\n",
    "- evolutionary algo - https://www.cs.vu.nl/~gusz/ecbook/Eiben-Smith-Intro2EC-Ch2.pdf\n",
    "- evolutionary rl - https://github.com/RichardAragon/EvolutionaryReinforcementLearning\n",
    "- paper to get games, metrics, algos - https://github.com/shiivashaakeri/Pong-Deep-QLearning/blob/main/Report.pdf\n",
    "- q-learning tutorial - https://www.geeksforgeeks.org/machine-learning/q-learning-in-python/\n",
    "- book implementations for RL - https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\n",
    "- RL taxonomy - https://github.com/bennylp/RL-Taxonomy\n",
    "- OpenAI RL taxonomy - https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\n",
    "- SARSA - https://github.com/linesd/tabular-methods/blob/master/algorithms/temporal_difference.py; https://www.geeksforgeeks.org/machine-learning/sarsa-reinforcement-learning/\n",
    "- ALL RL ALGOS - https://github.com/FareedKhan-dev/all-rl-algorithms/blob/master/cheatsheet.md\n",
    "- RL algorithms paper - https://arxiv.org/pdf/2209.14940\n",
    "- minmax q-learning - https://github.com/tocom242242/minimax_q_learning/blob/master/minimax_q_learner.py; https://github.com/theevann/MinimaxQ-Learning\n",
    "- Multi-Step Minimax Q-learning Algorithm for Two-Player Zero-Sum Markov Games - https://arxiv.org/abs/2407.04240\n",
    "- Policies in MDPS and games - https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf\n",
    "- **FLATLAND challenge** - https://gitlab.aicrowd.com/flatland/flatland; https://www.youtube.com/watch?v=cvkeWwDQr0A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18cb80-924f-438b-83c6-68111ade10e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6aad0f-3bc4-4430-a246-24520a2c1f89",
   "metadata": {},
   "source": [
    "# Environments\n",
    "\n",
    "## 1. Rock-Paper-Scissors - RPS\n",
    "\n",
    "To test the agents, we implement a simple Rock Paper Scissors - RPS environment. In each iteration, the agents play the same game - repeated.\n",
    "\n",
    "### States\n",
    "In its simple version, the same state is repeated in each iteration.\n",
    "\n",
    "In its stochastic version, the state has a transition probability into the next state. In the next state, the rewards are doubled.\n",
    "\n",
    "1. `State-i`: Initial state: Standard rewards (+1 win, -1 loss)\n",
    "2. `State-n`: Next state: Rewards of previous state are doubled each time\n",
    "3. We cap the states to a number to reduce the amount of memory for the agents\n",
    "\n",
    "### Stochastic transition (P)\n",
    "- If there's a draw at any state, there's a P-chance to transition into next state in which rewards double.\n",
    "- Otherwise, game stays in the same state.\n",
    "\n",
    "### Zero-Sum rewards (R)\n",
    "- The sum of the rewards for Player 1 and Player 2 is always 0.\n",
    "- One player wins, other loses: rewards for one is equal penalty for other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86135a70-58b4-4985-b993-3e5472d2853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RPS_environment():\n",
    "    \"\"\"\n",
    "    Rock Paper Scissors environment - simple.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payoff_matrix=None, stochastic=False, transition_prob=0.3, max_states=3):\n",
    "\n",
    "        # Payoff Matrix: Player 1 rows, Player 2 cols\n",
    "        # 0: Rock\n",
    "        # 1: Paper\n",
    "        # 2: Scissors\n",
    "        # self.matrix = [\n",
    "        #         # R  P  S\n",
    "        #         [0, -1, 1],  # R\n",
    "        #         [1, 0, -1],  # P\n",
    "        #         [-1, 1, 0],  # S\n",
    "        #         ]\n",
    "        \n",
    "        self.payoff_matrix = payoff_matrix\n",
    "        self.stochastic = stochastic\n",
    "        self.state = 0\n",
    "        self.P = transition_prob\n",
    "        self.max_states = max_states\n",
    "        self.score1 = 0\n",
    "        self.score2 = 0\n",
    "        self.reward1 = 0\n",
    "        self.reward2 = 0\n",
    "        self.draws = 0\n",
    "\n",
    "    def step(self, action1, action2):\n",
    "        \"\"\"\n",
    "        Transition function:\n",
    "        1. receive actions from players\n",
    "        2. calculate rewards\n",
    "        3. update state\n",
    "        4. return information\n",
    "        \"\"\"\n",
    "\n",
    "        reward1 = self.payoff_matrix[action1][action2]*(self.state+1)\n",
    "        reward2 = -reward1\n",
    "\n",
    "        self.update_score(reward1, reward2)\n",
    "        self.update_reward(reward1, reward2)\n",
    "\n",
    "        # Calculate transition\n",
    "        if self.stochastic:\n",
    "            if action1 == action2:\n",
    "                if random.random() < self.P:\n",
    "                    if self.state < self.max_states:\n",
    "                        # We transition\n",
    "                        self.state += 1\n",
    "            else:\n",
    "                self.reset()\n",
    "\n",
    "        return self.state, reward1, reward2\n",
    "\n",
    "    def update_score(self, reward1, reward2):\n",
    "        \"\"\"Keep scores of the agents.\"\"\"\n",
    "        if reward1>reward2:\n",
    "            self.score1+=1\n",
    "        elif reward2>reward1:\n",
    "            self.score2+=1\n",
    "        else:\n",
    "            self.draws+=1\n",
    "\n",
    "    def update_reward(self, reward1, reward2):\n",
    "        \"\"\"Reward counter for each agent.\"\"\"\n",
    "        self.reward1 += reward1\n",
    "        self.reward2 += reward2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Winning condition reached.\"\"\"\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb753e1-343f-4872-a3d2-d5fa29c58e7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Agents\n",
    "\n",
    "## Fictitious play - FP agent\n",
    "\n",
    "- This is a model based agent - tries to build a model of the environment to predict the best move.\n",
    "- It has a memory of the previous plays of the opponent - tracks state changes.\n",
    "- It tracks its opponents moves with a counter: `counts`\n",
    "- It tracks a distribution for each of the opponent's moves: `sigma`\n",
    "- In each round it plays the opposite move of the opponent: `best_move`\n",
    "- This agent can get trapped into playing non-optimal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80962388-bc4d-412c-a783-db033cd3a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FictitiousPlayAgent():\n",
    "    \"\"\"\n",
    "    Fictitious Play Agent: A model based agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payoff_matrix):\n",
    "        # Times a move has been played\n",
    "                      #R #P #S\n",
    "        self.counts = [1, 1, 1]\n",
    "\n",
    "        # Probability distribution\n",
    "                      #R #P #S\n",
    "        self.sigma = [0, 0, 0]\n",
    "\n",
    "        # Utility matrix\n",
    "        self.rows = 3\n",
    "        self.cols = 3\n",
    "        self.payoff_matrix = payoff_matrix\n",
    "\n",
    "    def action(self, opponent_action):\n",
    "        \"\"\"\n",
    "        Calculate the best action and ACT.\n",
    "        \"\"\"\n",
    "        if opponent_action is not None:\n",
    "            self.update_counts(opponent_action)\n",
    "            self.update_sigma(opponent_action)\n",
    "            best_move = self.find_best_move()\n",
    "        else:\n",
    "            best_move = random.randint(0, 2)\n",
    "        return best_move\n",
    "\n",
    "    def update_counts(self, opponent_action):\n",
    "        self.counts[opponent_action] += 1\n",
    "\n",
    "    def update_sigma(self, opponent_action):\n",
    "        for i in range(len(self.sigma)):\n",
    "            self.sigma[i] = round(self.counts[i]/sum(self.counts), 3)\n",
    "\n",
    "    def find_best_move(self):\n",
    "        \"\"\"\n",
    "        Calculates utilities using distribution sigma and returns best move.\n",
    "        \"\"\"\n",
    "        max_u = -float(\"inf\")\n",
    "        best_move = random.randint(0, 2)\n",
    "        for i in range(self.rows):\n",
    "            row_i_sum = sum([a*b for a, b in zip(self.payoff_matrix[i], self.sigma)])\n",
    "            if row_i_sum > max_u:\n",
    "                max_u = row_i_sum\n",
    "                best_move = i\n",
    "\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab462b94-784d-494f-bcae-0a5e419097cc",
   "metadata": {},
   "source": [
    "## Q-learning agent - simple\n",
    "\n",
    "The simplest version of Q-learning algorithm.\n",
    "\n",
    "This agent keep a table for each of its actions. It dcides on the next action using Bellman's equation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334a3b60-d9b8-42a5-8765-d5539352a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearning():\n",
    "    \"\"\"Generic Q-Learning algorightm.\"\"\"\n",
    "\n",
    "    def __init__(self, states=[0], actions=[0, 1, 2]):\n",
    "        # Initialize Q-table\n",
    "        self.q_table = {state: [0.0 for action in actions] for state in states}\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.action_space = actions\n",
    "\n",
    "    def action(self, state):\n",
    "        \"\"\"\n",
    "        Choose an action using the epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # Exploire - Exploit\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state_values = self.q_table[state]\n",
    "            max_val = max(state_values)\n",
    "            # If we have multiple max values, runs a loop\n",
    "            best_action = [i for i, v in enumerate(state_values) if v == max_val]\n",
    "            return np.random.choice(best_action)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Update the Q-value using Bellman eq..\n",
    "        \"\"\"\n",
    "        max_next_q = max(self.q_table[next_state])\n",
    "        # TEmpoeral difference\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "\n",
    "        # New Q value\n",
    "        self.q_table[state][action] += self.alpha * (td_target - self.q_table[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf229d9-42e8-427f-beac-88ead2c3d18a",
   "metadata": {},
   "source": [
    "# Scenarios\n",
    "\n",
    "## FP self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c19bfbea-9073-48ab-a64c-17f68cb81f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_self_play(rounds):\n",
    "    env = RPS_environment(\n",
    "        payoff_matrix=[\n",
    "                 # R  P  S\n",
    "                 [0, -1, 1],  # R\n",
    "                 [1, 0, -1],  # P\n",
    "                 [-1, 1, 0],  # S\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Row player\n",
    "    agent1 = FictitiousPlayAgent(\n",
    "            [\n",
    "                 #R  P  S\n",
    "                [0, -1, 1],  # R\n",
    "                [1, 0, -1],  # P\n",
    "                [-1, 1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Column player\n",
    "    agent2 = FictitiousPlayAgent(\n",
    "            [\n",
    "                #R  P  S\n",
    "                [0, 1, -1],  # R\n",
    "                [-1, 0, 1],  # P\n",
    "                [1, -1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Initializer\n",
    "    last_act2 = None\n",
    "    last_act1 = None\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(rounds):\n",
    "        curr_act1 = agent1.action(last_act2)\n",
    "        curr_act2 = agent2.action(last_act1)\n",
    "\n",
    "        env.step(curr_act1, curr_act2)\n",
    "\n",
    "        last_act1 = curr_act1\n",
    "        last_act2 = curr_act2\n",
    "\n",
    "    print(f\"Agent 1\\ncounts: {agent1.counts}\\nsigma: {agent1.sigma}\\nScore: {env.score1}\\nTotal reward: {env.reward1}\")\n",
    "    print(\"----------------\")\n",
    "    print(f\"Agent 2\\ncounts: {agent2.counts}\\nsigma: {agent2.sigma}\\nScore: {env.score2}\\nTotal reward: {env.reward2}\")\n",
    "    print(f\"Draws: {env.draws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "282167c3-e5e8-47b4-bad5-471d870520aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1\n",
      "counts: [72, 29, 1]\n",
      "sigma: [0.706, 0.284, 0.01]\n",
      "Score: 58\n",
      "Total reward: 44\n",
      "----------------\n",
      "Agent 2\n",
      "counts: [1, 86, 15]\n",
      "sigma: [0.01, 0.843, 0.147]\n",
      "Score: 14\n",
      "Total reward: -44\n",
      "Draws: 28\n"
     ]
    }
   ],
   "source": [
    "fp_self_play(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553e84b-a351-428a-a716-e79d14fd90fb",
   "metadata": {},
   "source": [
    "## FP vs Q-learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "706cc878-1eaa-4389-9005-c4746807961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_vs_qlearning(rounds):\n",
    "    \n",
    "    env = RPS_environment(\n",
    "        payoff_matrix=[\n",
    "                 # R  P  S\n",
    "                 [0, -1, 1],  # R\n",
    "                 [1, 0, -1],  # P\n",
    "                 [-1, 1, 0],  # S\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Row agent\n",
    "    agent1 = FictitiousPlayAgent(\n",
    "            [\n",
    "                 #R  P  S\n",
    "                [0, -1, 1],  # R\n",
    "                [1, 0, -1],  # P\n",
    "                [-1, 1, 0],  # S\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    agent2 = QLearning()\n",
    "\n",
    "    # Initializer\n",
    "    last_act2 = None\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    for i in range(rounds):\n",
    "        curr_env_state = env.state\n",
    "\n",
    "        # Register actions\n",
    "        curr_act1 = agent1.action(last_act2)\n",
    "        curr_act2 = agent2.action(curr_env_state) # pass curr env state\n",
    "\n",
    "        # Step the environment\n",
    "        next_env_state, reward1, reward2 = env.step(curr_act1, curr_act2)\n",
    "\n",
    "        # RL agent learns\n",
    "        agent2.update_q_value(curr_env_state, curr_act2, reward2, next_env_state)\n",
    "\n",
    "        last_act2 = curr_act2\n",
    "\n",
    "    print(f\"FP\\ncounts: {agent1.counts}\\nsigma: {agent1.sigma}\\nScore: {env.score1}\\nTotal reward: {env.reward1}\")\n",
    "    print(\"------------\")\n",
    "    print(f\"Q-learning\\nq_table: {agent2.q_table}\\nScore: {env.score2}\\nTotal reward: {env.reward2}\")\n",
    "    print(f\"Draws: {env.draws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a7a06c04-ca0d-440a-99af-7781ea7e057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP\n",
      "counts: [332, 332, 338]\n",
      "sigma: [0.331, 0.331, 0.337]\n",
      "Score: 139\n",
      "Total reward: -23\n",
      "------------\n",
      "Q-learning\n",
      "q_table: {0: [0.41565003889968577, 1.2906433197566063, 0.8524037548223204]}\n",
      "Score: 162\n",
      "Total reward: 23\n",
      "Draws: 699\n"
     ]
    }
   ],
   "source": [
    "fp_vs_qlearning(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543150d0-e528-45c3-9490-ca08748fc22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f12b9-7bc6-44f3-b5c6-74c09db34bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a24ec-409b-4ffd-b7a9-a9b3124c8004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
